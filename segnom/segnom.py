from argparse import ArgumentParser
import matplotlib.pyplot as plt
import numpy as np
import os
from PIL import Image
from sklearn.manifold import TSNE
import torch
from transformers import AutoModel, AutoImageProcessor

from sam2.build_sam import build_sam2
from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator

def show_anns(anns, borders=True):
    """
    Visualize the annotations (masks) generated by SAM2.

    This function takes a list of annotations and displays them as overlays on a matplotlib plot.
    Each annotation is represented by a semi-transparent colored mask. Optionally, borders can be
    drawn around each segmented region.

    Args:
        anns (list): A list of dictionaries, where each dictionary represents an annotation
                     and contains at least 'segmentation' and 'area' keys.
        borders (bool, optional): If True, draws borders around each segmented region. 
                                  Defaults to True.

    Returns:
        None
    """
    if len(anns) == 0: return
    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)
    ax = plt.gca()
    ax.set_autoscale_on(False)
    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))
    img[:, :, 3] = 0
    for ann in sorted_anns:
        m = ann['segmentation']
        color_mask = np.concatenate([np.random.random(3), [0.5]])
        img[m] = color_mask 
        if borders:
            import cv2
            contours, _ = cv2.findContours(m.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) 
            contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]
            cv2.drawContours(img, contours, -1, (0, 0, 1, 0.4), thickness=1) 

    ax.imshow(img)

def save_segmentations(masks, original_image, original_image_name):
    """
    Save individual segmentations from a set of masks as separate image files.

    This function takes a list of masks (typically generated by a segmentation model)
    and the original image, then saves each segmented region as a separate image file.
    The segmented regions are cropped to their bounding boxes and saved with 
    transparent backgrounds where the mask is not applied.

    Args:
        masks (list): A list of dictionaries, each containing a 'segmentation' key
                      with a boolean numpy array representing the mask.
        original_image (PIL.Image.Image): The original image from which segmentations
                                          were generated.

    Returns:
        None
    """
    original_array = np.array(original_image)    
    for i, mask in enumerate(masks):
        bool_mask = mask['segmentation'].astype(bool)
        rows, cols = np.where(bool_mask)
        top, left = rows.min(), cols.min()
        bottom, right = rows.max(), cols.max()
        cropped_image = original_array[top:bottom+1, left:right+1]
        cropped_mask = bool_mask[top:bottom+1, left:right+1]
        seg_image = np.zeros_like(cropped_image)
        seg_image[cropped_mask] = cropped_image[cropped_mask]
        pil_seg_image = Image.fromarray(seg_image)
        segmentation_folder = os.path.join('segmentations', original_image_name)
        os.makedirs(segmentation_folder, exist_ok=True)
        pil_seg_image.save(f'{segmentation_folder}/segmentation_{i+1}.png')

if __name__=="__main__":
    parser = ArgumentParser(description="Process images for segmentation.")
    parser.add_argument('--image', type=str, required=True, help='Path to the image file')
    parser.add_argument('--device', type=str, required=False, help='Torch device', default='mps')
    args = parser.parse_args()
    device = torch.device(args.device)
    print(f"using device: {device}")
    test_image = Image.open(args.image)
    test_image_name = os.path.splitext(os.path.basename(args.image))[0]

    # load nomic image encoder
    image_nomencoder = AutoModel.from_pretrained("nomic-ai/nomic-embed-vision-v1.5", trust_remote_code=True)
    image_preprocessor = AutoImageProcessor.from_pretrained("nomic-ai/nomic-embed-vision-v1.5")

    # load SAM model
    sam2_checkpoint = "../../sam2/checkpoints/sam2.1_hiera_large.pt"
    model_cfg = "configs/sam2.1/sam2.1_hiera_l.yaml"
    sam2 = build_sam2(model_cfg, sam2_checkpoint, device=device, apply_postprocessing=False)
    mask_generator = SAM2AutomaticMaskGenerator(sam2)

    # generate masks for the segmented objects
    masks = mask_generator.generate(np.array(test_image.convert("RGB")))

    # save segmentations to folder
    save_segmentations(masks, test_image, test_image_name)
    print(f"Saved {len(masks)} segmentation images in segmentations/{test_image_name}")

    
    image_filenames = sorted([f for f in os.listdir(f"segmentations/{test_image_name}") if f.endswith(".png")])
    segmentation_embeddings = []
    for filename in image_filenames:
        print(f"Encoding frame {filename}")
        img = Image.open(f"segmentations/{test_image_name}/{filename}")
        img_emb = image_nomencoder(**image_preprocessor(img, return_tensors="pt"))
        segmentation_embeddings.append(torch.nn.functional.normalize(img_emb.last_hidden_state[:, 0], p=2, dim=1).detach().numpy().flatten())
    np.save(f"segmentations/{test_image_name}/gram.npy", segmentation_embeddings)

    segmentation_embeddings = np.load(f"segmentations/{test_image_name}/gram.npy")
    test_image_embedding = image_nomencoder(**image_preprocessor(test_image, return_tensors="pt"))
    test_image_embedding = torch.nn.functional.normalize(test_image_embedding.last_hidden_state[:, 0], p=2, dim=1).detach().numpy().flatten()
    all_embeddings = np.vstack([test_image_embedding, segmentation_embeddings])

    tsne = TSNE(n_components=2, random_state=42, perplexity=5)
    embeddings_2d = tsne.fit_transform(all_embeddings)

    x, y = embeddings_2d[:, 0], embeddings_2d[:, 1]
    plt.figure(figsize=(10, 8))
    plt.scatter(x[1:], y[1:], c='blue', s=30, alpha=0.6, label='Segmentations')
    plt.scatter(x[0], y[0], c='red', s=200, marker='*', label='Original Image')
    plt.title(f"t-SNE visualization of image embeddings for {test_image_name}")
    plt.xlabel("t-SNE feature 1")
    plt.ylabel("t-SNE feature 2")
    plt.legend()
    plt.savefig(f"segmentations/{test_image_name}/tsne_plot.png")
    plt.close()
    print(f"t-SNE plot saved as segmentations/{test_image_name}/tsne_plot.png")